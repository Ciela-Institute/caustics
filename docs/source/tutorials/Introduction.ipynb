{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import vmap\n",
    "from torch.func import jacfwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to Caustics!\n",
    "\n",
    "In this introduction, we will showcase some of the features and design principles of Caustics. We will see\n",
    "1. How to get started from one of our pre-built `Simulator`\n",
    "\n",
    "2. Visualization of the `Simulator` graph (DAG of `caustics` modules)\n",
    "\n",
    "3. Distinction between **Static** and **Dynamic** parameters\n",
    "\n",
    "4. How to create a a **batch** of simulations\n",
    "\n",
    "5. Semantic structure of the Simulator input\n",
    "6. Taking gradient w.r.t. to parameters with `Pytorch` autodiff functionalities\n",
    "7. Swapping in flexible modules like the `Pixelated` representation for more advanced usage\n",
    "8. How to create your own Simulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started with the `LensSource` Simulator\n",
    "\n",
    "For this first introduction, we use the simplest modules in caustics for the lens and source, namely the `SIE` and the `Sersic` modules. We also assume a `FlatLambdaCDM` cosmology. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from caustics import LensSource, SIE, Sersic, FlatLambdaCDM\n",
    "\n",
    "# Define parameters of the camera pixel grid\n",
    "pixelscale = 0.04  # arcsec/pixel\n",
    "pixels = 100\n",
    "\n",
    "# Instantiate modules for the simulator\n",
    "cosmo = FlatLambdaCDM(name=\"cosmo\")\n",
    "lens = SIE(cosmology=cosmo, name=\"lens\")\n",
    "source = Sersic(name=\"source\")\n",
    "simulator = LensSource(lens, source, pixelscale=pixelscale, pixels_x=pixels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating a simulation of a strong gravitational lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "z_s = torch.tensor([1.0])\n",
    "lens_params = torch.tensor(\n",
    "    [\n",
    "        0.5,  # z_l\n",
    "        0.0,  # x_0\n",
    "        0.0,  # y_0\n",
    "        0.9,  # q\n",
    "        0.4,  # phi\n",
    "        1.0,  # b (Einstein radius)\n",
    "    ]\n",
    ")\n",
    "\n",
    "source_params = torch.tensor(\n",
    "    [\n",
    "        0.0,  # x_0\n",
    "        0.0,  # y_0\n",
    "        0.5,  # q\n",
    "        0.9,  # phi\n",
    "        1.0,  # n\n",
    "        0.1,  # Re\n",
    "        10.0,  # I_e\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a lensed image\n",
    "y = simulator([z_s, lens_params, source_params])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# A meshgrid to show the source\n",
    "x = torch.linspace(-0.5, 0.5, 100)\n",
    "X, Y = torch.meshgrid(x, x, indexing=\"xy\")\n",
    "\n",
    "ax = axs[0]\n",
    "ax.set_title(r\"SÃ©rsic source\")\n",
    "source_im = source.brightness(X, Y, source_params)\n",
    "ax.imshow(source_im, origin=\"lower\", extent=(-0.5, 0.5, -0.5, 0.5), cmap=\"gray\")\n",
    "ax.set_ylabel(r\"$\\beta_y$ ['']\")\n",
    "ax.set_xlabel(r\"$\\beta_x$ ['']\")\n",
    "\n",
    "ax = axs[1]\n",
    "ax.set_title(r\"SIE mass distribution\")\n",
    "lens_im = lens.convergence(X * 2, Y * 2, z_s, lens_params)\n",
    "ax.imshow(lens_im, origin=\"lower\", extent=(-1, 1, -1, 1), cmap=\"hot\")\n",
    "ax.set_ylabel(r\"$\\theta_y$ ['']\")\n",
    "ax.set_xlabel(r\"$\\theta_x$ ['']\")\n",
    "\n",
    "ax = axs[2]\n",
    "ax.set_title(r\"Lensed image\")\n",
    "ax.imshow(y, origin=\"lower\", extent=(-1, 1, -1, 1), cmap=\"gray\")\n",
    "ax.set_ylabel(r\"$\\theta_y$ ['']\")\n",
    "ax.set_xlabel(r\"$\\theta_x$ ['']\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of the `Simulator` DAG \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulator.graph(True, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Static** vs **Dynamic** parameters\n",
    "\n",
    "In the DAG shown above, \n",
    "\n",
    "- **Dynamic parameters** are shown in white boxes\n",
    "\n",
    "- **Static parameters** are shown in grey boxes \n",
    "\n",
    "The distinction between the two types can be summarized as follows\n",
    "\n",
    "- **Dynamic parameters** are fed as input to the simulator and can be batched over (data parallelism)\n",
    "\n",
    "- **Static parameters** have fixed values. Their values is stored in the internal DAG, and will be broadcasted over when batching computation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a parameter static\n",
    "simulator.z_s = 1.0\n",
    "simulator.graph(False, True)  # z_s turns grey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a parameter dynamic\n",
    "simulator.z_s = None\n",
    "simulator.graph(\n",
    "    False, True\n",
    ")  # z_s turns white, which makes it disappear when we don't show the dynamic parameters (first option False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulating a batch of observations\n",
    "\n",
    "We use `vmap` over the simulator to create a batch of parameters. In this example, we create a batch of examples that only differ by their Einstein radius. To do this, we turn all the other parameter into static parameters. This is done in the hidden cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Make all parameters static except the Einstein radius\n",
    "simulator.lens.x0 = 0.0\n",
    "simulator.lens.y0 = 0.0\n",
    "simulator.lens.q = 0.9\n",
    "simulator.lens.phi = 0.4\n",
    "simulator.lens.b = None  # Make sure this one stays Dynamic\n",
    "simulator.lens.z_l = 0.5\n",
    "\n",
    "simulator.source.x0 = 0.0\n",
    "simulator.source.y0 = 0.0\n",
    "simulator.source.q = 0.5\n",
    "simulator.source.phi = 0.5\n",
    "simulator.source.n = 1.0\n",
    "simulator.source.Re = 0.1\n",
    "simulator.source.Ie = 10.0\n",
    "\n",
    "simulator.z_s = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import vmap\n",
    "\n",
    "# Create a grid of Einstein radius\n",
    "b = torch.linspace(0.5, 1.5, 5).view(-1, 1)  # Shape is [B, 1]\n",
    "ys = vmap(simulator)(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 5, figsize=(20, 4))\n",
    "\n",
    "for i, ax in enumerate(axs.flatten()):\n",
    "    ax.axis(\"off\")\n",
    "    ax.imshow(ys[i], cmap=\"gray\")\n",
    "    ax.set_title(f\"$b = {b[i].item():.2f}$\")\n",
    "plt.subplots_adjust(wspace=0, hspace=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic structure of the input\n",
    "\n",
    "The simulator's input takes different format to allow different usecase scenarios\n",
    "1. Flattened tensor for deep neural network like in [Hezaveh et al. (2017)](https://arxiv.org/abs/1708.08842)\n",
    "\n",
    "2. Semantic List to separate the input int terms of high level modules like Lens and Source\n",
    "3. Low-level Dictionary to decompose the parameters at the level of the leafs of the DAG\n",
    "\n",
    "Below, we illustrate how to use all of these structures. For completeness, we also use `vmap`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make some parameters dynamic for this example\n",
    "simulator.source.Ie = None\n",
    "simulator.lens.b = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flattened Tensor\n",
    "To make sure the order of the parameter is correct, print the simulator. Order of dynamic parameters is shown in the `x_order` field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 5  # Batch dimension\n",
    "b = torch.rand(B, 1)\n",
    "Ie = torch.rand(B, 1)\n",
    "x = torch.concat([b, Ie], dim=1)  # Concat along the feature dimension\n",
    "\n",
    "# Now we can use vmap to simulate multiple images at once\n",
    "ys = vmap(simulator)(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic lists\n",
    "\n",
    "A semantic list is simply a list over module parameters like the one we used earlier: `[z_s, lens_params, source_params]`. Note that we could also include cosmological parameters in that list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Make some parameters dynamic for this example\n",
    "simulator.source.Ie = None\n",
    "simulator.lens.b = None\n",
    "simulator.lens.x0 = None\n",
    "simulator.lens.cosmology.h0 = None\n",
    "\n",
    "simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 5\n",
    "lens_params = torch.randn(B, 2)  # x0 and b\n",
    "source_params = torch.rand(B, 1)  # Ie\n",
    "cosmo_params = torch.rand(B, 1)  # h0\n",
    "\n",
    "x = [lens_params, cosmo_params, source_params]\n",
    "ys = vmap(simulator)(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low-level Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "B = 5\n",
    "x0 = torch.randn(B, 1)\n",
    "b = torch.randn(B, 1)\n",
    "Ie = torch.rand(B, 1)\n",
    "h0 = torch.rand(B, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = {\n",
    "    \"lens\": {\n",
    "        \"x0\": x0,\n",
    "        \"b\": b,\n",
    "    },\n",
    "    \"source\": {\n",
    "        \"Ie\": Ie,\n",
    "    },\n",
    "    \"cosmo\": {\n",
    "        \"h0\": h0,\n",
    "    },\n",
    "}\n",
    "ys = vmap(simulator)(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing gradients with automatic differentiation\n",
    "\n",
    "Computing gradients is particularly useful for optimization. Since taking gradients w.r.t. list or dictionary inputs is not possible with `torch.func.grad`, we will need a small wrapper around the simulator. For optimisation, the wrapper will often be a log likelihood function. For now we use a generic `lambda` wrapper. \n",
    "\n",
    "In the case of the semantic list input, the wrapper has the general form\n",
    "```python\n",
    "lambda *x: simulator(x)\n",
    "```\n",
    "\n",
    "The low-level dictionary input is a bit more involved but can be worked out on a case by case basis. \n",
    "\n",
    "**Note**: apply `vmap` around the gradient function (e.g. `jacfwd` or `grad`) to handle batched computation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Choose some sensible values to compute the gradient\n",
    "lens_params = torch.tensor([0.0, 1.0])  # x0 and b\n",
    "source_params = torch.tensor([10.0])  # Ie\n",
    "cosmo_params = torch.tensor([0.7])  # h0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`jacfwd` will return a list of 3 tensors of shape [B, pixels, pixels, D], where D is the number of parameters in that module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.func import jacfwd\n",
    "\n",
    "jac = jacfwd(lambda *x: simulator(x), argnums=(0, 1, 2))(\n",
    "    lens_params, cosmo_params, source_params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 4, figsize=(20, 4))\n",
    "\n",
    "titles = [\n",
    "    r\"$\\nabla_{x_0} f(\\mathbf{x})$\",\n",
    "    r\"$\\nabla_{b} f(\\mathbf{x})$\",\n",
    "    r\"$\\nabla_{h_0} f(\\mathbf{x})$\",\n",
    "    r\"$\\nabla_{I_e} f(\\mathbf{x})$\",\n",
    "]\n",
    "jacs = torch.concat(jac, dim=-1)\n",
    "for i, ax in enumerate(axs.flatten()):\n",
    "    ax.axis(\"off\")\n",
    "    ax.imshow(jacs[..., i], cmap=\"seismic\", vmin=-10, vmax=10)\n",
    "    ax.set_title(titles[i], fontsize=18);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pixelated representations\n",
    "\n",
    "The examples above made use of very simplistic modules. Here, we will showcase how easily we can swap-in flexible representations to represent more realistic systems. \n",
    "\n",
    "- `Pixelated` is the module used to represent the background source with a grid of pixels\n",
    "\n",
    "- `PixelatedConvergence` is the module used to represent the convergence of the lens with a grid of pixels\n",
    "\n",
    "For this example, we will use source samples from the PROBES dataset ([Stone et al., 2019](https://iopscience.iop.org/article/10.3847/1538-4357/ab3126/meta#:~:text=The%20intrinsic%20scatter%20of%20the%20baryonic%20RAR%20is%20predicted%20by,null%20value%20reported%20by%20L17.)) and convergence maps sampled from Illustris TNG ([Nelson et al., 2019](https://comp-astrophys-cosmol.springeropen.com/articles/10.1186/s40668-019-0028-x), see [Adam et al., 2023](https://iopscience.iop.org/article/10.3847/1538-4357/accf84/meta) for preprocessing, or use this [link](https://zenodo.org/records/6555463/files/hkappa128hst_TNG100_rau_trainset.h5?download=1) to download the maps). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from caustics import Pixelated, PixelatedConvergence\n",
    "\n",
    "# Some static parameters for the simulator\n",
    "pixelscale = 0.07\n",
    "source_pixelscale = 0.25 * pixelscale\n",
    "z_l = 0.5\n",
    "z_s = 1.0\n",
    "x0 = 0\n",
    "y0 = 0\n",
    "\n",
    "# Construct the Simulator with Pixelated and PixalatedConvergence modules\n",
    "cosmo = FlatLambdaCDM(name=\"cosmo\")\n",
    "source = Pixelated(\n",
    "    name=\"source\", shape=(256, 256), pixelscale=source_pixelscale, x0=x0, y0=y0\n",
    ")\n",
    "lens = PixelatedConvergence(\n",
    "    cosmology=cosmo,\n",
    "    name=\"lens\",\n",
    "    pixelscale=pixelscale,\n",
    "    shape=(128, 128),\n",
    "    z_l=z_l,\n",
    ")\n",
    "simulator = LensSource(lens, source, pixelscale=pixelscale, pixels_x=pixels, z_s=z_s)\n",
    "\n",
    "simulator.graph(True, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the hidden cell below, we load the maps from a dataset. If you downloaded the datasets mentioned above, you can use the code below to load maps from them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# import h5py\n",
    "\n",
    "# B = 10\n",
    "# path_to_kappa_maps = \"/path/to/hkappa128hst_TNG100_rau_trainset.h5\"  # modify this to your system path\n",
    "# index = [250] + sorted(list(np.random.randint(251, 1000, size=B-1)))\n",
    "# kappa_map = torch.tensor(h5py.File(path_to_kappa_maps, \"r\")[\"kappa\"][index])\n",
    "\n",
    "# path_to_source_maps = \"/path/to/probes.h5\"  # modify this to your system path\n",
    "# index = [101] + sorted(list(np.random.randint(251, 1000, size=B-1)))\n",
    "# filter_ = 0  # grz filters: 0 is g, etc.\n",
    "# source_map = torch.tensor(\n",
    "#     h5py.File(path_to_source_maps, \"r\")[\"galaxies\"][index, ..., filter_]\n",
    "# )\n",
    "\n",
    "# Load saved assets for demonstration\n",
    "kappa_maps = torch.tensor(\n",
    "    np.load(\"assets/kappa_maps.npz\", allow_pickle=True)[\"kappa_maps\"]\n",
    ")\n",
    "source_maps = torch.tensor(\n",
    "    np.load(\"assets/source_maps.npz\", allow_pickle=True)[\"source_maps\"]\n",
    ")\n",
    "\n",
    "# Cherry picked example\n",
    "source_map = source_maps[0]\n",
    "kappa_map = kappa_maps[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a simulation by feeding the maps as input to the simulator (using semantic list inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = simulator([kappa_map, source_map])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "beta_extent = [\n",
    "    -source_pixelscale * source_map.shape[0] / 2,\n",
    "    source_pixelscale * source_map.shape[0] / 2,\n",
    "] * 2\n",
    "\n",
    "ax = axs[0]\n",
    "ax.set_title(r\"Source map\")\n",
    "ax.imshow(source_map, origin=\"lower\", cmap=\"gray\", extent=beta_extent)\n",
    "ax.set_ylabel(r\"$\\beta_y$ ['']\")\n",
    "ax.set_xlabel(r\"$\\beta_x$ ['']\")\n",
    "\n",
    "theta_extent = [-pixelscale * pixels / 2, pixelscale * pixels / 2] * 2\n",
    "\n",
    "ax = axs[1]\n",
    "ax.set_title(r\"Convergence map\")\n",
    "ax.imshow(\n",
    "    kappa_map,\n",
    "    origin=\"lower\",\n",
    "    cmap=\"hot\",\n",
    "    extent=theta_extent,\n",
    "    norm=plt.cm.colors.LogNorm(vmin=1e-1, vmax=10),\n",
    ")\n",
    "ax.set_ylabel(r\"$\\theta_y$ ['']\")\n",
    "ax.set_xlabel(r\"$\\theta_x$ ['']\")\n",
    "ax.set_title(r\"Convergence map\")\n",
    "\n",
    "ax = axs[2]\n",
    "ax.set_title(r\"Lensed image\")\n",
    "ax.imshow(y, origin=\"lower\", extent=theta_extent, cmap=\"gray\")\n",
    "ax.set_ylabel(r\"$\\theta_y$ ['']\")\n",
    "ax.set_xlabel(r\"$\\theta_x$ ['']\")\n",
    "ax.set_title(r\"Lensed image\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, batching works the same way as before and is super fast. Below, we show the time it takes to make 4 batched simulations on a laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "ys = vmap(simulator)([kappa_maps, source_maps])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 3, figsize=(9, 9))\n",
    "\n",
    "ys = vmap(simulator)([kappa_maps, source_maps])\n",
    "for i in range(3):\n",
    "    ax = axs[i, 0]\n",
    "    ax.axis(\"off\")\n",
    "    ax.imshow(\n",
    "        source_maps[len(ys) - 1 - i],\n",
    "        origin=\"lower\",\n",
    "        cmap=\"gray\",\n",
    "        norm=plt.cm.colors.LogNorm(vmin=1e-2, vmax=1, clip=True),\n",
    "    )\n",
    "\n",
    "    ax = axs[i, 1]\n",
    "    ax.axis(\"off\")\n",
    "    ax.imshow(\n",
    "        kappa_maps[len(ys) - 1 - i],\n",
    "        origin=\"lower\",\n",
    "        cmap=\"hot\",\n",
    "        norm=plt.cm.colors.LogNorm(vmin=1e-1, vmax=10),\n",
    "    )\n",
    "\n",
    "    ax = axs[i, 2]\n",
    "    ax.axis(\"off\")\n",
    "    ax.imshow(\n",
    "        ys[len(ys) - 1 - i],\n",
    "        origin=\"lower\",\n",
    "        cmap=\"gray\",\n",
    "        norm=plt.cm.colors.LogNorm(vmin=1e-2, vmax=1, clip=True),\n",
    "    )\n",
    "axs[0, 0].set_title(r\"Source map\")\n",
    "axs[0, 1].set_title(r\"Convergence map\")\n",
    "axs[0, 2].set_title(r\"Lensed image\")\n",
    "plt.subplots_adjust(wspace=0, hspace=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating your own Simulator\n",
    "\n",
    "Here, we only introduce the general design principles to create a Simulator. Worked examples can be found in [this notebook](./Simulators.ipynb). \n",
    "\n",
    "### A Simulator is very much like a neural network in Pytorch\n",
    "A simulator inherits from the super class `Simulator`, similar to how a neural network inherits from the `nn.Module` class in `Pytorch`\n",
    "\n",
    "```python\n",
    "from caustics import Simulator\n",
    "\n",
    "class MySim(Simulator):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        ...\n",
    "```\n",
    "\n",
    "- The `init` method constructs the computation graph, initialize the `caustics` modules, and can prepare or store variables for the `forward` method. \n",
    "- The `forward` method is where the actual simulation happens.\n",
    "- `x` generally denotes a set of parameters which affect the computations in the simulator graph. \n",
    "\n",
    "### How to use a Simulator in your workflow\n",
    "Like a neural network, `MySim` (and in general any `caustics` modules), must be instantiated **outside** the main workload. This is because `caustics` builds a graph internally every time a module is created. Ideally, this happens only once to avoid overhead. In general, you can follow the following code pattern\n",
    "```python\n",
    "\n",
    "# Instantiation\n",
    "simulator = MySim()\n",
    "\n",
    "# Heavy workload\n",
    "for n in range(N):\n",
    "    y = vmap(simulator)(x)\n",
    "```\n",
    "This allows you to perform inefficient computations that only need to happen once in the `__init__` method while keeping your forward method lightweight.\n",
    "\n",
    "### How to feed parameters to the different modules\n",
    "\n",
    "This is probably the easiest part of building a Simulator. Just feed `x` at the end of each method. And you are done. \n",
    " \n",
    " Here is a minimal example that shows how to feed the parameters `x` to different modules in the `forward` method\n",
    " ```python\n",
    " def forward(self, x):\n",
    "    alpha_x, alpha_y = self.lens.reduced_deflection_angle(self.theta_x, self.theta_y, self.z_s, x)\n",
    "    beta_x = self.theta_x - alpha_x # lens equation\n",
    "    ...\n",
    "    lensed_image = self.source.brightness(beta_x, beta_y, x)\n",
    " ``` \n",
    "\n",
    "You might worry that `x` can have a relatively complex structure (flattened tensor, semantic lict, low-level dictionary). \n",
    "`caustics` handles this complexity for you. \n",
    "You only need to make sure that `x` contains all the **dynamic** parameters required by your custom simulator. \n",
    "This design works for every `caustics` module and each of their methods, meaning that `x` is always the last argument in a `caustics` method call signature.  \n",
    " \n",
    "The only details that you need to handle explicitly in your own simulator are stuff like the camera pixel position (`theta_x` and `theta_y`), and source redshifts (`z_s`). Those are often constructed in the `__init__` method because they can be assumed fixed. Thus, the example above assumed that they can be retrieved from the `self` registry. A Simulator is often an abstraction of an instrument with many fixed variables to describe it, or aimed at a specific observation. \n",
    "\n",
    "Of course, you could have more complex workflows for which this assumption is not true. For example, you might want to infer the PSF parameters of your instrument and need to feed this to the simulator as a dynamic parameter. \n",
    "The next section has what you need to customize completely your simulator\n",
    "\n",
    "### Creating your own variables as leafs in the DAG\n",
    "\n",
    "You can register new variables in the DAG for custom calculations as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from caustics import Simulator\n",
    "\n",
    "\n",
    "class MySim(Simulator):\n",
    "    def __init__(self):\n",
    "        super().__init__()  # Don't forget to use super!!\n",
    "        # shape has to be a tuple, e.g. shape=(1,). This can be any shape you need.\n",
    "        self.add_param(\n",
    "            \"my_dynamic_arg\", value=None, shape=(1,)\n",
    "        )  # register a dynamic parameter in the DAG\n",
    "        self.add_param(\n",
    "            \"my_static_arg\", value=1.0, shape=(1,)\n",
    "        )  # register a static parameter in the DAG\n",
    "\n",
    "    def forward(self, x):\n",
    "        my_arg = x[\"MySim\"][0]  # retrieve your arguments\n",
    "        my_arg2 = x[\"MySim\"][1]\n",
    "\n",
    "        # My very complex workflow\n",
    "        ...\n",
    "\n",
    "\n",
    "sim = MySim()\n",
    "sim.graph(True, True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "caustic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
