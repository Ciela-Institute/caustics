{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Modelling a Lens image using MCMC\n",
    "\n",
    "In this hypothetical scenario we have an image of galaxy galaxy strong lensing and we would like to recover a model of this scene. Thus we will need to determine parameters for the background source light, the lensing galaxy light, and the lensing galaxy mass distribution. A common technique for analyzing strong lensing systems is a Markov Chain Monte-Carlo which can explore the parameter space and provide us with important metrics about the model and uncertainty on all parameters. Since caustics is differentiable we have access to especially efficient gradient based MCMC algorithms. First, we will demo a classical MCMC algorithm (using emcee) on the problem and show how for this high dimensional problem the autocorrelation length is high. Next we will show how just adding a bit of gradient information (via MALA) will significantly improve results by reducing autocorrelation length. Finally, we will demo NUTS on the problem, which is highly gradient based and is convenient in that it can be run with no tunable parameters, it's autocorrelation length is generally approximately 1. However, to achieve this, NUTS needs to run many steps internally, which ultimately means that even though it requires more tweaking to set up, MALA, is often more efficient. The best algorithm for your use case will depend on a number of factors, here you will see how caustics can play well with any sampling algorithm available, giving you lots of flexibility!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import caustics\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colormaps\n",
    "from matplotlib.patches import Ellipse\n",
    "from scipy.stats import norm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import MCMC as pyro_MCMC\n",
    "from pyro.infer import NUTS as pyro_NUTS\n",
    "import emcee"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Specs for the data\n",
    "\n",
    "These are some properties of the data that aren't very interesting for the demo, it includes the size of the image, pixelscale, noise level, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Data specs\n",
    "\n",
    "background_rms = 0.005  #  background noise per pixel\n",
    "exp_time = 500.0  #  exposure time (arbitrary units, flux per pixel is in units #photons/exp_time unit)\n",
    "numPix = 60  #  cutout pixel size per axis\n",
    "pixelscale = 0.05  #  pixel size in arcsec (area per pixel = pixel_scale**2)\n",
    "fwhm = 0.05  # full width at half maximum of PSF\n",
    "psf_sigma = fwhm / (2 * np.sqrt(2 * np.log(2)))\n",
    "psf_type = \"GAUSSIAN\"  # 'GAUSSIAN', 'PIXEL', 'NONE'\n",
    "\n",
    "cosmology = caustics.FlatLambdaCDM(name=\"cosmo\")\n",
    "cosmology.to(dtype=torch.float32)\n",
    "\n",
    "upsample_factor = 1\n",
    "quad_level = 3\n",
    "thx, thy = caustics.utils.meshgrid(\n",
    "    pixelscale / upsample_factor,\n",
    "    upsample_factor * numPix,\n",
    "    dtype=torch.float32,\n",
    ")\n",
    "z_l = torch.tensor(0.5, dtype=torch.float32)\n",
    "z_s = torch.tensor(1.5, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Build simulator forward model\n",
    "\n",
    "Here we build the caustics simulator which will handle the lensing and generating our images for the sake of fitting. It includes a model for the lens mass distribution, lens light, and source light. We also include a simple gaussian PSF for extra realism, though for simplicity we will use the same PSF model for simulating the mock data and fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lens mass model (SIE + shear)\n",
    "lens_sie = caustics.SIE(name=\"galaxylens\", cosmology=cosmology, z_l=1.0)\n",
    "lens_shear = caustics.ExternalShear(\n",
    "    name=\"externalshear\", cosmology=cosmology, x0=0.0, y0=0.0, z_l=1.0\n",
    ")\n",
    "lens_mass_model = caustics.SinglePlane(\n",
    "    name=\"lensmass\", cosmology=cosmology, lenses=[lens_sie, lens_shear], z_l=1.0\n",
    ")\n",
    "\n",
    "# Lens light model (sersic)\n",
    "lens_light_model = caustics.Sersic(name=\"lenslight\")\n",
    "\n",
    "# Source light model (sersic)\n",
    "source_light_model = caustics.Sersic(name=\"sourcelight\")\n",
    "\n",
    "# Gaussian PSF Model\n",
    "psf_image = caustics.utils.gaussian(\n",
    "    nx=upsample_factor * 6 + 1,\n",
    "    ny=upsample_factor * 6 + 1,\n",
    "    pixelscale=pixelscale / upsample_factor,\n",
    "    sigma=psf_sigma,\n",
    "    upsample=2,\n",
    ")\n",
    "\n",
    "# Image plane simulator\n",
    "sim = caustics.LensSource(\n",
    "    lens=lens_mass_model,\n",
    "    lens_light=lens_light_model,\n",
    "    source=source_light_model,\n",
    "    psf=psf_image,\n",
    "    pixels_x=numPix,\n",
    "    pixelscale=pixelscale,\n",
    "    upsample_factor=upsample_factor,\n",
    "    z_s=2.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Sample some mock data\n",
    "\n",
    "Here we write out the true values for all the parameters in the model. In total there are 21 parameters, so this is quite a complex model already! We then plot the data so we can see what it is we re trying to fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Generate the mock data\n",
    "true_params = {\n",
    "    \"galaxylens\": {\n",
    "        \"x0\": 0.05,\n",
    "        \"y0\": 0.0,\n",
    "        \"q\": 0.86,\n",
    "        \"phi\": -0.20,\n",
    "        \"b\": 0.66,\n",
    "    },\n",
    "    \"externalshear\": {\"gamma_1\": 0.0, \"gamma_2\": -0.05},\n",
    "    \"sourcelight\": {\n",
    "        \"x0\": 0.1,\n",
    "        \"y0\": 0.0,\n",
    "        \"q\": 0.75,\n",
    "        \"phi\": 1.18,\n",
    "        \"n\": 1.0,\n",
    "        \"Re\": 0.1 / np.sqrt(0.75),\n",
    "        \"Ie\": 16 * pixelscale**2,\n",
    "    },\n",
    "    \"lenslight\": {\n",
    "        \"x0\": 0.05,\n",
    "        \"y0\": 0.0,\n",
    "        \"q\": 0.75,\n",
    "        \"phi\": 1.18,\n",
    "        \"n\": 2.0,\n",
    "        \"Re\": 0.6 / np.sqrt(0.75),\n",
    "        \"Ie\": 16 * pixelscale**2,\n",
    "    },\n",
    "}\n",
    "allparams = []\n",
    "for model in true_params:\n",
    "    for key in true_params[model]:\n",
    "        allparams.append(true_params[model][key])\n",
    "allparams = torch.tensor(allparams)\n",
    "print(true_params)\n",
    "\n",
    "# simulate lens, crop extra evaluation for PSF\n",
    "true_system = sim(allparams)  # simulate at high resolution\n",
    "\n",
    "fig, axarr = plt.subplots(1, 2, figsize=(15, 8))\n",
    "axarr[0].imshow(\n",
    "    np.log10(true_system.detach().cpu().numpy()), cmap=\"inferno\", origin=\"lower\"\n",
    ")\n",
    "axarr[0].axis(\"off\")\n",
    "axarr[0].set_title(\"Mock Lens System\")\n",
    "torch.manual_seed(42)\n",
    "shot_noise = torch.normal(\n",
    "    mean=torch.zeros_like(true_system),\n",
    "    std=torch.sqrt(torch.abs(true_system) / exp_time),\n",
    ")\n",
    "background = torch.normal(\n",
    "    mean=torch.zeros_like(true_system), std=torch.tensor(background_rms)\n",
    ")\n",
    "variance = (torch.abs(true_system) / exp_time) + background_rms**2\n",
    "obs_system = true_system + shot_noise + background\n",
    "print(((obs_system - true_system) ** 2 / variance).sum().item() / 3600)\n",
    "axarr[1].imshow(\n",
    "    np.log10(obs_system.abs().detach().cpu().numpy()), cmap=\"inferno\", origin=\"lower\"\n",
    ")\n",
    "axarr[1].axis(\"off\")\n",
    "axarr[1].set_title(\"Mock Observation\")\n",
    "# plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "def corner_plot(\n",
    "    chain,\n",
    "    labels=None,\n",
    "    figsize=(10, 10),\n",
    "    true_values=None,\n",
    "    ellipse_colors=\"g\",\n",
    "):\n",
    "    num_params = chain.shape[1]\n",
    "    cov_matrix = np.cov(chain, rowvar=False)\n",
    "    mean = np.mean(chain, axis=0)\n",
    "    fig, axes = plt.subplots(num_params, num_params, figsize=figsize)\n",
    "    plt.subplots_adjust(wspace=0.0, hspace=0.0)\n",
    "\n",
    "    for i in range(num_params):\n",
    "        for j in range(num_params):\n",
    "            ax = axes[i, j]\n",
    "\n",
    "            if i == j:\n",
    "                x = np.linspace(\n",
    "                    mean[i] - 3 * np.sqrt(cov_matrix[i, i]),\n",
    "                    mean[i] + 3 * np.sqrt(cov_matrix[i, i]),\n",
    "                    100,\n",
    "                )\n",
    "                y = norm.pdf(x, mean[i], np.sqrt(cov_matrix[i, i]))\n",
    "                ax.plot(x, y, color=\"g\")\n",
    "                ax.set_xlim(\n",
    "                    mean[i] - 3 * np.sqrt(cov_matrix[i, i]),\n",
    "                    mean[i] + 3 * np.sqrt(cov_matrix[i, i]),\n",
    "                )\n",
    "                if true_values is not None:\n",
    "                    ax.axvline(true_values[i], color=\"red\", linestyle=\"-\", lw=1)\n",
    "            elif j < i:\n",
    "                ax.scatter(chain[:, j], chain[:, i], color=\"c\", s=0.1, zorder=0)\n",
    "                cov = cov_matrix[np.ix_([j, i], [j, i])]\n",
    "                lambda_, v = np.linalg.eig(cov)\n",
    "                lambda_ = np.sqrt(lambda_)\n",
    "                angle = np.rad2deg(np.arctan2(v[1, 0], v[0, 0]))\n",
    "                for k in [1, 2]:\n",
    "                    ellipse = Ellipse(\n",
    "                        xy=(mean[j], mean[i]),\n",
    "                        width=lambda_[0] * k * 2,\n",
    "                        height=lambda_[1] * k * 2,\n",
    "                        angle=angle,\n",
    "                        edgecolor=ellipse_colors,\n",
    "                        facecolor=\"none\",\n",
    "                    )\n",
    "                    ax.add_artist(ellipse)\n",
    "\n",
    "                # Set axis limits\n",
    "                margin = 3\n",
    "                ax.set_xlim(\n",
    "                    mean[j] - margin * np.sqrt(cov_matrix[j, j]),\n",
    "                    mean[j] + margin * np.sqrt(cov_matrix[j, j]),\n",
    "                )\n",
    "                ax.set_ylim(\n",
    "                    mean[i] - margin * np.sqrt(cov_matrix[i, i]),\n",
    "                    mean[i] + margin * np.sqrt(cov_matrix[i, i]),\n",
    "                )\n",
    "\n",
    "                if true_values is not None:\n",
    "                    ax.axvline(true_values[j], color=\"red\", linestyle=\"-\", lw=1)\n",
    "                    ax.axhline(true_values[i], color=\"red\", linestyle=\"-\", lw=1)\n",
    "\n",
    "            if j > i:\n",
    "                ax.axis(\"off\")\n",
    "\n",
    "            if i < num_params - 1:\n",
    "                ax.set_xticklabels([])\n",
    "            else:\n",
    "                if labels is not None:\n",
    "                    ax.set_xlabel(labels[j])\n",
    "            ax.yaxis.set_major_locator(plt.NullLocator())\n",
    "\n",
    "            if j > 0:\n",
    "                ax.set_yticklabels([])\n",
    "            else:\n",
    "                if labels is not None:\n",
    "                    ax.set_ylabel(labels[i])\n",
    "            ax.xaxis.set_major_locator(plt.NullLocator())\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd256dff",
   "metadata": {},
   "source": [
    "## Fit using emcee\n",
    "\n",
    "We now model the data using emcee which handles standard Metropolis-Hastings MCMC sampling (plus a few tricks). First we need to construct a log likelihood function. In our case this is just the squared residuals, divided by the variance in each pixel. The rest is specific to the emcee implementation. Note that we must use many walkers due to the algorithm emcee uses, since this is a 21 dimensional problem we need at least 42 chains and we use 64 since that is a nice power of 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make batched simulator\n",
    "vsim = torch.vmap(sim)\n",
    "\n",
    "\n",
    "# Log-likelihood function\n",
    "def density(x):\n",
    "    model = vsim(torch.as_tensor(x))\n",
    "    log_likelihood_value = -0.5 * torch.sum(\n",
    "        ((model - obs_system) ** 2) / variance, dim=(1, 2)\n",
    "    )\n",
    "    log_likelihood_value = torch.nan_to_num(log_likelihood_value, nan=-np.inf)\n",
    "    return log_likelihood_value.numpy()\n",
    "\n",
    "\n",
    "nwalkers = 64\n",
    "ndim = len(allparams)\n",
    "\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, density, vectorize=True)\n",
    "\n",
    "x0 = allparams + 0.01 * torch.randn(nwalkers, ndim)\n",
    "print(\"burn-in\")\n",
    "state = sampler.run_mcmc(x0, 100, skip_initial_state_check=True)  # burn-in\n",
    "sampler.reset()\n",
    "print(\"production\")\n",
    "state = sampler.run_mcmc(state, 1000)  # production"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ed9291",
   "metadata": {},
   "source": [
    "We have taken 64000 samples in this demo, in general you would want many more (each chain needs to run longer than 1000 steps in order to fully mix). Its always a good idea to plot the chains and check that they don't have any pathological features (i.e. getting frozen at one value). We subtract the mean and divide by the standard deviation of each parameter so that the chains can all be plotted together despite having very different values. Here we can see the non zero autocorrelation length for one of the chains even over 1000 steps. This indicates we should run the chains much longer, but this is just a demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d87abb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_mh = sampler.get_chain()\n",
    "normed_chains = (chain_mh[:, 0] - np.mean(chain_mh[:, 0], axis=0)) / np.std(\n",
    "    chain_mh[:, 0], axis=0\n",
    ")\n",
    "for i in range(chain_mh.shape[2]):\n",
    "    plt.plot(normed_chains[:, i], color=colormaps[\"viridis\"](i / chain_mh.shape[2]))\n",
    "plt.title(\"Chain for each parameter\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480e849f",
   "metadata": {},
   "source": [
    "Since the autocorrelation length is >1, we can compute an effective sample size to determine how many equivalent independent points we have drawn. As the warning suggests, in this demo we cannot compute the actual autocorrelation length, the autocorrelation length increases as we draw more samples (you can test this by changing the 1000 above to a larger number). Assuming that the autocorrelation is actually of a similar length to the chain (1000), this means we have drawn approximately 64 independent samples (one for each walker)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b0924f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Autocorrelation time: \",\n",
    "    np.mean(emcee.autocorr.integrated_time(chain_mh, quiet=True)),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51c577b",
   "metadata": {},
   "source": [
    "We may plot the samples in a corner plot. However, we thin the samples first so that the number of points is not overwhelming to plot. As you can see in the subfigures there is still a bloby structure of the samples, suggesting that the chains were not run long enough to converge and fill the probability volume.\n",
    "\n",
    "In this figure the green contours show the covariance matrix computed from the samples, the cyan points are the samples themselves, and the red lines are ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afae21ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = chain_mh.shape[0] * chain_mh.shape[1]\n",
    "fig = corner_plot(\n",
    "    np.concatenate(chain_mh, axis=0)[:: int(N / 200)],\n",
    "    true_values=allparams.numpy(),\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4a2be6",
   "metadata": {},
   "source": [
    "# Fit with MALA sampling\n",
    "\n",
    "Metropolis Adjusted Langevin Algorithm (MALA) sampling is the half way point between NUTS and MH, it uses gradient information to make an efficient proposal distribution for a MH step. We have written a basic implementation below for demo purposes. Essentially, one uses a random perturbation like in MH, except with a bias towards higher likelihood which comes from the gradient. Detailed balance is maintained using a MH step, so we still sample the correct distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a6de8f",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "def mala_sampler(\n",
    "    initial_state, log_prob, log_prob_grad, num_samples, epsilon, mass_matrix\n",
    "):\n",
    "    \"\"\"Metropolis Adjusted Langevin Algorithm (MALA) sampler with batch dimension.\n",
    "\n",
    "    Args:\n",
    "    - initial_state (numpy array): Initial states of the chains, shape (num_chains, dim).\n",
    "    - log_prob (function): Function to compute the log probabilities of the current states.\n",
    "    - log_prob_grad (function): Function to compute the gradients of the log probabilities.\n",
    "    - num_samples (int): Number of samples to generate.\n",
    "    - epsilon (float): Step size for the Langevin dynamics.\n",
    "    - mass_matrix (numpy array): Mass matrix, shape (dim, dim), used to scale the dynamics.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "    - samples (numpy array): Array of sampled values, shape (num_samples, num_chains, dim).\n",
    "    \"\"\"\n",
    "    num_chains, dim = initial_state.shape\n",
    "    samples = np.zeros((num_samples, num_chains, dim))\n",
    "    x_current = np.array(initial_state)\n",
    "    current_log_prob = log_prob(x_current)\n",
    "    inv_mass_matrix = np.linalg.inv(mass_matrix)\n",
    "    chol_inv_mass_matrix = np.linalg.cholesky(inv_mass_matrix)\n",
    "\n",
    "    pbar = tqdm(range(num_samples))\n",
    "    acceptance_rate = np.zeros([0])\n",
    "    for i in pbar:\n",
    "        gradients = log_prob_grad(x_current)\n",
    "        noise = np.dot(np.random.randn(num_chains, dim), chol_inv_mass_matrix.T)\n",
    "        proposal = (\n",
    "            x_current\n",
    "            + 0.5 * epsilon**2 * np.dot(gradients, inv_mass_matrix)\n",
    "            + epsilon * noise\n",
    "        )\n",
    "\n",
    "        # proposal = x_current + 0.5 * epsilon**2 * gradients + epsilon * np.random.randn(num_chains, *dim)\n",
    "        proposal_log_prob = log_prob(proposal)\n",
    "        # Metropolis-Hastings acceptance criterion, computed for each chain\n",
    "        acceptance_log_prob = proposal_log_prob - current_log_prob\n",
    "        accept = np.log(np.random.rand(num_chains)) < acceptance_log_prob\n",
    "        acceptance_rate = np.concatenate([acceptance_rate, accept])\n",
    "        pbar.set_description(f\"Acceptance rate: {acceptance_rate.mean():.2f}\")\n",
    "\n",
    "        # Update states where accepted\n",
    "        x_current[accept] = proposal[accept]\n",
    "        current_log_prob[accept] = proposal_log_prob[accept]\n",
    "\n",
    "        samples[i] = x_current\n",
    "\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e934cbb9",
   "metadata": {},
   "source": [
    "Here we run the MALA sampler after a small burn-in. We cheat a little bit and use the previous sampler to construct a mass matrix, this makes MALA more efficient but you could just as easily set the mass matrix to identity for the burn-in then use the burn-in samples to get a mass matrix, it only requires more fiddling with parameters (epsilon)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c53b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def density_grad(x):\n",
    "    x = torch.as_tensor(x, dtype=torch.float32)\n",
    "    x.requires_grad = True\n",
    "    model = vsim(x)\n",
    "    log_likelihood_value = -0.5 * torch.sum(\n",
    "        ((model - obs_system) ** 2) / variance, dim=(1, 2)\n",
    "    )\n",
    "    log_likelihood_value = torch.nan_to_num(log_likelihood_value, nan=-np.inf)\n",
    "    log_likelihood_value.sum().backward()\n",
    "    return x.grad.numpy()\n",
    "\n",
    "\n",
    "nwalkers = 32\n",
    "x0 = allparams + 0.01 * torch.randn(nwalkers, ndim)\n",
    "mass_matrix = np.linalg.inv(np.cov(chain_mh.reshape(-1, ndim), rowvar=False))\n",
    "\n",
    "chain_burnin_mala = mala_sampler(\n",
    "    initial_state=x0,\n",
    "    log_prob=density,\n",
    "    log_prob_grad=density_grad,\n",
    "    num_samples=100,\n",
    "    epsilon=3e-1,\n",
    "    mass_matrix=mass_matrix,\n",
    ")  # burn-in\n",
    "\n",
    "chain_mala = mala_sampler(\n",
    "    initial_state=chain_burnin_mala[-1],\n",
    "    log_prob=density,\n",
    "    log_prob_grad=density_grad,\n",
    "    num_samples=1000,\n",
    "    epsilon=3e-1,\n",
    "    mass_matrix=mass_matrix,\n",
    ")  # production"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e483c657",
   "metadata": {},
   "source": [
    "Plotting the chains we see that they mix much better than the MH sampler, but still have some autocorrelation, as would be expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fca7a07",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "normed_chains = (chain_mala[:, 0] - np.mean(chain_mala[:, 0], axis=0)) / np.std(\n",
    "    chain_mala[:, 0], axis=0\n",
    ")\n",
    "for i in range(chain_mala.shape[2]):\n",
    "    plt.plot(normed_chains[:, i], color=colormaps[\"viridis\"](i / chain_mala.shape[2]))\n",
    "plt.title(\"Chain for each parameter\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c8399f",
   "metadata": {},
   "source": [
    "The autocorrelation length is better than MH as expected. Again the effective sample size can't be trusted and is probably a bit larger than the 64 value from before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d058064a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Autocorrelation time: \",\n",
    "    np.mean(emcee.autocorr.integrated_time(chain_mala, quiet=True)),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c45f4f",
   "metadata": {},
   "source": [
    "The corner plot is much better than the MH example, we can see how the volume is filled out more with fewer gaps, suggesting we have sampled a good chunk of the space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e97dead",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "N = chain_mala.shape[0] * chain_mala.shape[1]\n",
    "fig = corner_plot(\n",
    "    np.concatenate(chain_mala, axis=0)[:: int(N / 200)],\n",
    "    true_values=allparams.numpy(),\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Fit using NUTS\n",
    "\n",
    "We now model the data using NUTS. A prior is required, so we just set some extremely wide values so that we will explore just the likelihood; in general one would want to pick more informative priors. The rest is specific to the Pyro NUTS implementation!\n",
    "\n",
    "Note, we use 25 warmup steps for Pyro, this is so it can automatically determine an appropriate \"mass matrix\" which helps the sampler explore much more efficiently! NUTS can also determine its own step size and build a full mass matrix. For a real analysis you would likely want to take many warm up steps so it can figure these things out accurately, afterwards the sampling will be incredibly efficient. For the sake of time in this demo, we preset a bunch of parameters and restrict what it can do, we will still get quite excellent samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "def step(model, prior):\n",
    "    x = pyro.sample(\"x\", prior)\n",
    "    # Log-likelihood function\n",
    "    res = model(x)\n",
    "    log_likelihood_value = -0.5 * torch.sum(((res - obs_system) ** 2) / variance)\n",
    "    # Observe the log-likelihood\n",
    "    pyro.factor(\"obs\", log_likelihood_value)\n",
    "\n",
    "\n",
    "prior = dist.Normal(\n",
    "    allparams,\n",
    "    torch.ones_like(allparams) * 1e2 + torch.abs(allparams) * 1e2,\n",
    ")\n",
    "nuts_kwargs = {\n",
    "    \"jit_compile\": True,\n",
    "    \"ignore_jit_warnings\": True,\n",
    "    \"step_size\": 2e-2,\n",
    "    \"full_mass\": False,\n",
    "    \"adapt_step_size\": False,\n",
    "    \"adapt_mass_matrix\": True,\n",
    "    \"target_accept_prob\": 0.8,\n",
    "    \"max_tree_depth\": 8,\n",
    "}\n",
    "\n",
    "nuts_kernel = pyro_NUTS(step, **nuts_kwargs)\n",
    "init_params = {\"x\": allparams}\n",
    "\n",
    "# Run MCMC with the NUTS sampler and the initial guess\n",
    "mcmc_kwargs = {\n",
    "    \"num_samples\": 100,\n",
    "    \"warmup_steps\": 25,\n",
    "    \"initial_params\": init_params,\n",
    "    \"disable_progbar\": False,\n",
    "}\n",
    "\n",
    "mcmc = pyro_MCMC(nuts_kernel, **mcmc_kwargs)\n",
    "\n",
    "mcmc.run(sim, prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "We have only taken 100 samples in this demo, in general you would want many more. Again we plot the chains and check that they look uncorrelated, everything seems fine here! There is much less structure than before, it looks like we are sampling random noise which is ideal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_nuts = mcmc.get_samples()[\"x\"]\n",
    "chain_nuts = chain_nuts.numpy()\n",
    "\n",
    "normed_chains = (chain_nuts - np.mean(chain_nuts, axis=0)) / np.std(chain_nuts, axis=0)\n",
    "for i in range(chain_nuts.shape[1]):\n",
    "    plt.plot(normed_chains[:, i], color=colormaps[\"viridis\"](i / chain_nuts.shape[1]))\n",
    "plt.title(\"Chain for each parameter\")\n",
    "plt.show()\n",
    "\n",
    "print(\n",
    "    \"Autocorrelation time: \",\n",
    "    np.mean(\n",
    "        emcee.autocorr.integrated_time(\n",
    "            chain_nuts, has_walkers=False, tol=10, quiet=True\n",
    "        )\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ebd65b",
   "metadata": {},
   "source": [
    "As is common for NUTS sampling, the average autocorrelation time for the parameters is around 1, meaning that essentially every sample is independent. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "Finally, we show the corner plot where the samples are very well distributed as we would expect for uncorrelated samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = corner_plot(chain_nuts, true_values=allparams.numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7428b8e",
   "metadata": {},
   "source": [
    "The Pyro NUTS implementation is very convenient in that it handles the tuning of all parameters automatically in the warmup phase. In practice however, the small but efficient steps of MALA often make it very efficient, plus the rapid progress makes it easier to tweak and tune the sampler to your problem. Emcee will struggle with high dimensional problems, it was already having a hard time at 21 dimensions and gravitational lensing analysis can go much beyond 21 parameters, but it doesn't require gradient computations which may be impractical for some forward models. In the end, your specific problem will likely determine which algorithm to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84bcb90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PY39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
