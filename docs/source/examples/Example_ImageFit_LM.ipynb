{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Fitting a Lens image using Levenberg-Marquardt\n",
    "\n",
    "In this hypothetical scenario we have an image of galaxy galaxy strong lensing and we would like to recover a model of this scene. Thus we will need to determine parameters for the background source light, the lensing galaxy light, and the lensing galaxy mass distribution. In this notebook we will assume the user has some method to find approximate parameters for all the models (perhaps guess and check by eye, a neural network, or a random number generator and a lot of computing power), once we are close to the optimal solution, Levenberg Maquardt can quickly converge to it. Note that LM will converge to a local minimum, so we need to make sure it's the right local minimum by giving it a good start!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import caustics\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Ellipse\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Specs for the data\n",
    "\n",
    "These are some properties of the data that aren't very interesting for the demo, it includes the size of the image, pixelscale, noise level, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data specs\n",
    "\n",
    "background_rms = 0.005  #  background noise per pixel\n",
    "exp_time = 1000.0  #  exposure time (arbitrary units, flux per pixel is in units #photons/exp_time unit)\n",
    "numPix = 60  #  cutout pixel size per axis\n",
    "pixelscale = 0.05  #  pixel size in arcsec (area per pixel = pixel_scale**2)\n",
    "fwhm = 0.05  # full width at half maximum of PSF\n",
    "psf_sigma = fwhm / (2 * np.sqrt(2 * np.log(2)))\n",
    "psf_type = \"GAUSSIAN\"  # 'GAUSSIAN', 'PIXEL', 'NONE'\n",
    "\n",
    "cosmology = caustics.FlatLambdaCDM(name=\"cosmo\")\n",
    "cosmology.to(dtype=torch.float32)\n",
    "\n",
    "upsample_factor = 1\n",
    "thx, thy = caustics.utils.get_meshgrid(\n",
    "    pixelscale / upsample_factor,\n",
    "    upsample_factor * numPix,\n",
    "    upsample_factor * numPix,\n",
    "    dtype=torch.float32,\n",
    ")\n",
    "z_l = torch.tensor(0.5, dtype=torch.float32)\n",
    "z_s = torch.tensor(1.5, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Build simulator forward model\n",
    "\n",
    "Here we build the caustics simulator which will handle the lensing and generating our images for the sake of fitting. It includes a model for the lens mass distribution, lens light, and source light. We also include a simple gaussian PSF for extra realism, though for simplicity we will use the same PSF model for simulating the mock data and fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the forward model\n",
    "\n",
    "# Lens mass model (SIE + shear)\n",
    "lens_sie = caustics.SIE(name=\"galaxylens\", cosmology=cosmology, z_l=1.0)\n",
    "lens_shear = caustics.ExternalShear(\n",
    "    name=\"externalshear\", cosmology=cosmology, x0=0.0, y0=0.0, z_l=1.0\n",
    ")\n",
    "lens_mass_model = caustics.SinglePlane(\n",
    "    name=\"lensmass\", cosmology=cosmology, lenses=[lens_sie, lens_shear], z_l=1.0\n",
    ")\n",
    "\n",
    "# Lens light model (sersic)\n",
    "lens_light_model = caustics.Sersic(name=\"lenslight\")\n",
    "\n",
    "# Source light model (sersic)\n",
    "source_light_model = caustics.Sersic(name=\"sourcelight\")\n",
    "\n",
    "# Gaussian PSF Model\n",
    "psf_image = caustics.utils.gaussian(\n",
    "    nx=upsample_factor * 6 + 1,\n",
    "    ny=upsample_factor * 6 + 1,\n",
    "    pixelscale=pixelscale / upsample_factor,\n",
    "    sigma=psf_sigma,\n",
    "    upsample=2,\n",
    ")\n",
    "\n",
    "# Image plane simulator\n",
    "sim = caustics.Lens_Source(\n",
    "    lens=lens_mass_model,\n",
    "    lens_light=lens_light_model,\n",
    "    source=source_light_model,\n",
    "    psf=psf_image,\n",
    "    pixels_x=numPix,\n",
    "    pixelscale=pixelscale,\n",
    "    upsample_factor=upsample_factor,\n",
    "    z_s=2.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Sample some mock data\n",
    "\n",
    "Here we write out the true values for all the parameters in the model. In total there are 21 parameters, so this is quite a complex model already! We then plot the data so we can see what it is we re trying to fit.\n",
    "\n",
    "Note that when we sample the simulator we call it with `quad_level=7`. This means the simulator will use gaussian quadrature sub-pixel integration to ensure the brightness of each pixel is very accurately computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the mock data\n",
    "true_params = {\n",
    "    \"galaxylens\": {\n",
    "        \"x0\": 0.05,\n",
    "        \"y0\": 0.0,\n",
    "        \"q\": 0.86,\n",
    "        \"phi\": -0.20,\n",
    "        \"b\": 0.66,\n",
    "    },\n",
    "    \"externalshear\": {\"gamma_1\": 0.0, \"gamma_2\": -0.05},\n",
    "    \"sourcelight\": {\n",
    "        \"x0\": 0.1,\n",
    "        \"y0\": 0.0,\n",
    "        \"q\": 0.75,\n",
    "        \"phi\": 1.18,\n",
    "        \"n\": 1.0,\n",
    "        \"Re\": 0.1 / np.sqrt(0.75),\n",
    "        \"Ie\": 16 * pixelscale**2,\n",
    "    },\n",
    "    \"lenslight\": {\n",
    "        \"x0\": 0.05,\n",
    "        \"y0\": 0.0,\n",
    "        \"q\": 0.75,\n",
    "        \"phi\": 1.18,\n",
    "        \"n\": 2.0,\n",
    "        \"Re\": 0.6 / np.sqrt(0.75),\n",
    "        \"Ie\": 16 * pixelscale**2,\n",
    "    },\n",
    "}\n",
    "\n",
    "allparams = []\n",
    "for model in true_params:\n",
    "    for key in true_params[model]:\n",
    "        allparams.append(true_params[model][key])\n",
    "allparams = torch.tensor(allparams)\n",
    "print(true_params)\n",
    "\n",
    "# simulate lens, crop extra evaluation for PSF\n",
    "true_system = sim(allparams, quad_level=7)  # simulate at high resolution\n",
    "\n",
    "fig, axarr = plt.subplots(1, 2, figsize=(15, 8))\n",
    "axarr[0].imshow(\n",
    "    np.log10(true_system.detach().cpu().numpy()), cmap=\"inferno\", origin=\"lower\"\n",
    ")\n",
    "axarr[0].axis(\"off\")\n",
    "axarr[0].set_title(\"Mock Lens System\")\n",
    "torch.manual_seed(42)\n",
    "shot_noise = torch.normal(\n",
    "    mean=torch.zeros_like(true_system),\n",
    "    std=torch.sqrt(torch.abs(true_system) / exp_time),\n",
    ")\n",
    "background = torch.normal(\n",
    "    mean=torch.zeros_like(true_system), std=torch.tensor(background_rms)\n",
    ")\n",
    "variance = (torch.abs(true_system) / exp_time) + background_rms**2\n",
    "obs_system = true_system + shot_noise + background\n",
    "print(((obs_system - true_system) ** 2 / variance).sum().item() / 3600)\n",
    "axarr[1].imshow(\n",
    "    np.log10(obs_system.detach().cpu().numpy()), cmap=\"inferno\", origin=\"lower\"\n",
    ")\n",
    "axarr[1].axis(\"off\")\n",
    "axarr[1].set_title(\"Mock Observation\")\n",
    "# plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Fit using Levenberg-Marquardt\n",
    "\n",
    "Since caustics is differentiable, it is very easy to write a Levenberg-Marquardt implementation (second order gradient descent). Caustics includes a basic implementation of LM though there are more sophisticated versions out there.\n",
    "\n",
    "To start we take the true parameters, copy them 10 times, and randomly perturb their values to simulate some process where we find close initial parameters for our model, but we haven't yet reached the maximum likelihood point. The fit itself only takes a minute to run all 10 starting points. In a real analysis you may be farther from the true parameters at initialization, but you could run hundreds or thoustands of starting points relatively cheaply to find the maximum likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_inits = allparams.repeat((10, 1))\n",
    "batch_inits += 0.01 * torch.randn_like(batch_inits)\n",
    "batch_inits = batch_inits.to(dtype=torch.float32)\n",
    "res = caustics.utils.batch_lm(\n",
    "    batch_inits,\n",
    "    obs_system.reshape(-1).repeat(10, 1),\n",
    "    lambda x: sim(x, quad_level=3).reshape(-1),\n",
    "    C=variance.reshape(-1).repeat(10, 1),\n",
    ")\n",
    "best_fit = res[0][np.argmin(res[2].numpy())]\n",
    "print(res[2] / np.prod(obs_system.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_fit, allparams)\n",
    "fig, axarr = plt.subplots(1, 2, figsize=(16, 8))\n",
    "axarr[0].imshow(\n",
    "    np.log10(sim(best_fit).detach().cpu().numpy()), origin=\"lower\", cmap=\"inferno\"\n",
    ")\n",
    "axarr[0].set_title(\"Fitted Lens System\")\n",
    "axarr[0].axis(\"off\")\n",
    "axarr[1].imshow(\n",
    "    ((obs_system - sim(best_fit)) / torch.sqrt(variance)).detach().cpu().numpy(),\n",
    "    cmap=\"bwr\",\n",
    "    vmin=-3,\n",
    "    vmax=3,\n",
    ")\n",
    "axarr[1].set_title(\"Residuals [(obs - fit)/std]\")\n",
    "axarr[1].axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Examine uncertainties\n",
    "\n",
    "A neat part about having a differentiable model is that we can easily compute derivatives and inspect our models. Below we compute the jacobian, which is a series of images that show how the model would change if we modified any of the parameters.\n",
    "\n",
    "The top row shows what would happen if the lens parameters were adjusted, the first 5 are SIE parameters and the last two are Shear parameters.\n",
    "\n",
    "The middle row shows how the image would change if we modified the source parameters, these represent a lensed Sersic profile.\n",
    "\n",
    "The bottom row shows how the image would change if we modified the lens light parameters, these represent an unlensed Sersic profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute jacobian\n",
    "J = torch.func.jacfwd(lambda x: sim(x, quad_level=3))(best_fit)\n",
    "fig, axarr = plt.subplots(3, 7, figsize=(21, 9))\n",
    "for i, ax in enumerate(axarr.flatten()):\n",
    "    ax.imshow(J[..., i], origin=\"lower\")\n",
    "    if i % 7 == 0:\n",
    "        ax.set_title([\"Lens\", \"Source\", \"Lenslight\"][i // 7])\n",
    "    ax.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "The code cell below uses a covariance matrix to construct a corner plot to display the full uncertainty matrix that we can compute for our model. More is explained below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corner_plot_covariance(\n",
    "    cov_matrix,\n",
    "    mean,\n",
    "    labels=None,\n",
    "    figsize=(10, 10),\n",
    "    true_values=None,\n",
    "    ellipse_colors=\"g\",\n",
    "):\n",
    "    num_params = cov_matrix.shape[0]\n",
    "    fig, axes = plt.subplots(num_params, num_params, figsize=figsize)\n",
    "    plt.subplots_adjust(wspace=0.0, hspace=0.0)\n",
    "\n",
    "    for i in range(num_params):\n",
    "        for j in range(num_params):\n",
    "            ax = axes[i, j]\n",
    "\n",
    "            if i == j:\n",
    "                x = np.linspace(\n",
    "                    mean[i] - 3 * np.sqrt(cov_matrix[i, i]),\n",
    "                    mean[i] + 3 * np.sqrt(cov_matrix[i, i]),\n",
    "                    100,\n",
    "                )\n",
    "                y = norm.pdf(x, mean[i], np.sqrt(cov_matrix[i, i]))\n",
    "                ax.plot(x, y, color=\"g\")\n",
    "                ax.set_xlim(\n",
    "                    mean[i] - 3 * np.sqrt(cov_matrix[i, i]),\n",
    "                    mean[i] + 3 * np.sqrt(cov_matrix[i, i]),\n",
    "                )\n",
    "                if true_values is not None:\n",
    "                    ax.axvline(true_values[i], color=\"red\", linestyle=\"-\", lw=1)\n",
    "            elif j < i:\n",
    "                cov = cov_matrix[np.ix_([j, i], [j, i])]\n",
    "                lambda_, v = np.linalg.eig(cov)\n",
    "                lambda_ = np.sqrt(lambda_)\n",
    "                angle = np.rad2deg(np.arctan2(v[1, 0], v[0, 0]))\n",
    "                for k in [1, 2]:\n",
    "                    ellipse = Ellipse(\n",
    "                        xy=(mean[j], mean[i]),\n",
    "                        width=lambda_[0] * k * 2,\n",
    "                        height=lambda_[1] * k * 2,\n",
    "                        angle=angle,\n",
    "                        edgecolor=ellipse_colors,\n",
    "                        facecolor=\"none\",\n",
    "                    )\n",
    "                    ax.add_artist(ellipse)\n",
    "\n",
    "                # Set axis limits\n",
    "                margin = 3\n",
    "                ax.set_xlim(\n",
    "                    mean[j] - margin * np.sqrt(cov_matrix[j, j]),\n",
    "                    mean[j] + margin * np.sqrt(cov_matrix[j, j]),\n",
    "                )\n",
    "                ax.set_ylim(\n",
    "                    mean[i] - margin * np.sqrt(cov_matrix[i, i]),\n",
    "                    mean[i] + margin * np.sqrt(cov_matrix[i, i]),\n",
    "                )\n",
    "\n",
    "                if true_values is not None:\n",
    "                    ax.axvline(true_values[j], color=\"red\", linestyle=\"-\", lw=1)\n",
    "                    ax.axhline(true_values[i], color=\"red\", linestyle=\"-\", lw=1)\n",
    "\n",
    "            if j > i:\n",
    "                ax.axis(\"off\")\n",
    "\n",
    "            if i < num_params - 1:\n",
    "                ax.set_xticklabels([])\n",
    "            else:\n",
    "                if labels is not None:\n",
    "                    ax.set_xlabel(labels[j])\n",
    "            ax.yaxis.set_major_locator(plt.NullLocator())\n",
    "\n",
    "            if j > 0:\n",
    "                ax.set_yticklabels([])\n",
    "            else:\n",
    "                if labels is not None:\n",
    "                    ax.set_ylabel(labels[i])\n",
    "            ax.xaxis.set_major_locator(plt.NullLocator())\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "For a $\\chi^2$ optimization problem it is possible to compute a very accurate approximation of the Hessian using just the Jacobian. Since caustics is autodifferentiable we have already easily extracted the Jacobian in a single line, so now we compute the Hessian as: $H \\approx J^T\\Sigma^{-1}J$ where $\\Sigma^{-1}$ is the inverse covariance matrix of pixel uncertainties. In our case we know the variance on each pixel so we simply divide by that. Finally the covariance matrix of uncertainties for our model parameters is just the matrix inverse of the Hessian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "J = J.reshape(-1, len(best_fit))\n",
    "# Compute Hessian\n",
    "H = J.T @ (J / variance.reshape(-1, 1))\n",
    "# Compute covariance matrix\n",
    "C = torch.linalg.inv(H)\n",
    "plt.imshow(np.log10(np.abs(C.detach().cpu().numpy())))\n",
    "plt.colorbar()\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Covariance matrix for parameters\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "A more helpful visual representation of the uncertainty covariance matrix is the corner plot below. For each parameter on the diagonal, and each pair of parameters on the lower triangle we now see how our fitted values and their uncertainties (green) align with the true parameters (red). As you can see, for the most part, the fitted values plus uncertainty enclose the true parameter. Note that these uncertainties are taken from a taylor expansion at the maximum likelihood and ultimately represent an approximation of the full uncertainty distribution. To fully explore the uncertainties one would need to run an MCMC sampling algorithm which can take a very long time before one will see the non-linear perturbations to the uncertainty in each parameter/pair. We have another notebook which does precisely this for the same mock setup!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "corner_plot_covariance(C, best_fit, true_values=allparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
